Model loaded..
Use LinearProbe head: pretrained_weights/10_16.pth
Choose layer: 23 for cls embedding
Not add Gaussian noise to the feature embedding.
Not use random erasing.
data mode: ours
[{'data_path': '/home/data/szk/TTO', 'data_mode': 'ours'}]
total_list_len: 2296
Fix the backbone.
Length of dataset: 144
Directory ./TTO_checkpoints/2024-10-16-19-44-55 is created.
Train loss: 0.014289895072579384 at step: 1
Iter time:  0.9431562423706055


Train loss: 0.007797800004482269 at step: 2
Iter time:  0.6193386316299438


Train loss: 0.011858908459544182 at step: 3
Iter time:  0.5021264553070068


Train loss: 0.010546058416366577 at step: 4
Iter time:  0.44356459379196167


Train loss: 0.010725002735853195 at step: 5
Iter time:  0.4085699558258057


Train loss: 0.014399446547031403 at step: 6
Iter time:  0.38531537850697833


Train loss: 0.02027653157711029 at step: 7
Iter time:  0.3688869135720389


Train loss: 0.010488729923963547 at step: 8
Iter time:  0.3566082715988159


Train loss: 0.02164013497531414 at step: 9
Iter time:  0.3469039334191216


Train loss: 0.016679814085364342 at step: 10
Iter time:  0.33906514644622804


Train loss: 0.0063813417218625546 at step: 11
Iter time:  0.33269201625477185


Train loss: 0.015169935300946236 at step: 12
Iter time:  0.3273765444755554


Train loss: 0.006921153515577316 at step: 13
Iter time:  0.32268102352435774


Train loss: 0.023749539628624916 at step: 14
Iter time:  0.31869142396109446


Train loss: 0.01693013682961464 at step: 15
Iter time:  0.31536092758178713


Train loss: 0.015039421617984772 at step: 16
Iter time:  0.3129652440547943


Train loss: 0.016263704746961594 at step: 17
Iter time:  0.31034263442544374


Train loss: 0.014909470453858376 at step: 18
Iter time:  0.3079879813724094


Train loss: 0.010297782719135284 at step: 19
Iter time:  0.3058923671120091


Train loss: 0.014436595141887665 at step: 20
Iter time:  0.3042724847793579


Train loss: 0.009857267141342163 at step: 21
Iter time:  0.30281821886698407


Train loss: 0.007646952755749226 at step: 22
Iter time:  0.30131466822190717


Train loss: 0.010771460831165314 at step: 23
Iter time:  0.2999570888021718


Train loss: 0.008957619778811932 at step: 24
Iter time:  0.298618088165919


Train loss: 0.008284240961074829 at step: 25
Iter time:  0.29771371841430666


Train loss: 0.007932726293802261 at step: 26
Iter time:  0.2968403651164128


Train loss: 0.008809056133031845 at step: 27
Iter time:  0.2958200949209708


Train loss: 0.01049503218382597 at step: 28
Iter time:  0.29494226830346243


Train loss: 0.013948660343885422 at step: 29
Iter time:  0.29410590796635067


Train loss: 0.011150680482387543 at step: 30
Iter time:  0.2932642380396525


Train loss: 0.012988408096134663 at step: 31
Iter time:  0.292512547585272


Train loss: 0.008294092491269112 at step: 32
Iter time:  0.2918082922697067


Train loss: 0.011020845733582973 at step: 33
Iter time:  0.29118050228465686


Train loss: 0.010767280124127865 at step: 34
Iter time:  0.29053842320161705


Train loss: 0.01170398760586977 at step: 35
Iter time:  0.289980867930821


Train loss: 0.012393226847052574 at step: 36
Iter time:  0.2894865605566237


Train loss: 0.0040922812186181545 at step: 37
Iter time:  0.28901725846367915


Train loss: 0.010842869058251381 at step: 38
Iter time:  0.2885263719056782


Train loss: 0.008840957656502724 at step: 39
Iter time:  0.2882520174368834


Train loss: 0.006929745897650719 at step: 40
Iter time:  0.28785308003425597


Train loss: 0.010190249420702457 at step: 41
Iter time:  0.2874261518804038


Train loss: 0.008995486423373222 at step: 42
Iter time:  0.28699300970349995


Train loss: 0.007066124118864536 at step: 43
Iter time:  0.2866380214691162


Train loss: 0.013135621324181557 at step: 44
Iter time:  0.2863207080147483


Train loss: 0.010527068749070168 at step: 45
Iter time:  0.2859772523244222


Train loss: 0.01187154557555914 at step: 46
Iter time:  0.28563776223555853


Train loss: 0.010040540248155594 at step: 47
Iter time:  0.2853200080546927


Train loss: 0.008892519399523735 at step: 48
Iter time:  0.2850629935661952


Train loss: 0.01206969004124403 at step: 49
Iter time:  0.2848074241560333


Train loss: 0.006746520288288593 at step: 50
Iter time:  0.28451809406280515


Train loss: 0.017055576667189598 at step: 51
Iter time:  0.28427329250410494


Train loss: 0.007422859780490398 at step: 52
Iter time:  0.2840100022462698


Train loss: 0.010559557005763054 at step: 53
Iter time:  0.28376855940189


Train loss: 0.008142602629959583 at step: 54
Iter time:  0.28351819956744156


Train loss: 0.00752244284376502 at step: 55
Iter time:  0.28329433527859776


Train loss: 0.010619253851473331 at step: 56
Iter time:  0.2831107590879713


Train loss: 0.007196133024990559 at step: 57
Iter time:  0.28289855990493507


Train loss: 0.008480178192257881 at step: 58
Iter time:  0.2826965307367259


Train loss: 0.011193382553756237 at step: 59
Iter time:  0.2824962219949496


Train loss: 0.008589599281549454 at step: 60
Iter time:  0.2822946826616923


Train loss: 0.005916999187320471 at step: 61
Iter time:  0.2822235803135106


Train loss: 0.007146498188376427 at step: 62
Iter time:  0.28203805415861066


Train loss: 0.0064590140245854855 at step: 63
Iter time:  0.2819148623754108


Train loss: 0.014190034940838814 at step: 64
Iter time:  0.2817593738436699


Train loss: 0.003966936841607094 at step: 65
Iter time:  0.28160049365117


Train loss: 0.0077300346456468105 at step: 66
Iter time:  0.28152509891625604


Train loss: 0.010852478444576263 at step: 67
Iter time:  0.2813658963388471


Train loss: 0.0064713540486991405 at step: 68
Iter time:  0.28123609108083386


Train loss: 0.01126907393336296 at step: 69
Iter time:  0.2810927957728289


Train loss: 0.009218892082571983 at step: 70
Iter time:  0.28096653393336707


Train loss: 0.0035477280616760254 at step: 71
Iter time:  0.28082732415535083


Train loss: 0.006382034160196781 at step: 72
Iter time:  0.2806902296013302


Train loss: 0.005387671757489443 at step: 73
Iter time:  0.28058638964613825


Train loss: 0.0038839257322251797 at step: 74
Iter time:  0.28047162133294185


Train loss: 0.007669707760214806 at step: 75
Iter time:  0.2803607145945231


Train loss: 0.008813993073999882 at step: 76
Iter time:  0.2802400965439646


Train loss: 0.006418850272893906 at step: 77
Iter time:  0.2801335724917325


Train loss: 0.007034276612102985 at step: 78
Iter time:  0.2800244520872067


Train loss: 0.008513791486620903 at step: 79
Iter time:  0.27992976164516015


Train loss: 0.007700891699641943 at step: 80
Iter time:  0.27981383800506593


Train loss: 0.012912978418171406 at step: 81
Iter time:  0.27969695609292866


Train loss: 0.009723860770463943 at step: 82
Iter time:  0.2795952674819202


Train loss: 0.004416648764163256 at step: 83
Iter time:  0.2794889915420348


Train loss: 0.009068126790225506 at step: 84
Iter time:  0.27939232758113314


Train loss: 0.009475097060203552 at step: 85
Iter time:  0.2793039882884306


Train loss: 0.006529792211949825 at step: 86
Iter time:  0.27922806628914765


Train loss: 0.004889633506536484 at step: 87
Iter time:  0.2791941412563982


Train loss: 0.009526897221803665 at step: 88
Iter time:  0.27913744341243396


Train loss: 0.007500878535211086 at step: 89
Iter time:  0.2790866889310687


Train loss: 0.007899229414761066 at step: 90
Iter time:  0.2790184259414673


Train loss: 0.008958590216934681 at step: 91
Iter time:  0.2789454617343106


Train loss: 0.005441085435450077 at step: 92
Iter time:  0.278873842695485


Train loss: 0.005650254897773266 at step: 93
Iter time:  0.2788204018787671


Train loss: 0.00865157786756754 at step: 94
Iter time:  0.27875136314554416


Train loss: 0.007215431891381741 at step: 95
Iter time:  0.2787512553365607


Train loss: 0.006816328968852758 at step: 96
Iter time:  0.27867598334948224


Train loss: 0.010626636445522308 at step: 97
Iter time:  0.2786023321839952


Train loss: 0.0033605562057346106 at step: 98
Iter time:  0.2785307923141791


Train loss: 0.008647375740110874 at step: 99
Iter time:  0.27846882319209554


Train loss: 0.008175849914550781 at step: 100
Iter time:  0.27841188669204714


Train loss: 0.006594441365450621 at step: 101
Iter time:  0.27835193010840087


Train loss: 0.009098945185542107 at step: 102
Iter time:  0.2782875720192404


Train loss: 0.011752050369977951 at step: 103
Iter time:  0.27822563254717486


Train loss: 0.005054568871855736 at step: 104
Iter time:  0.2781734076830057


Train loss: 0.005693274550139904 at step: 105
Iter time:  0.2781183878580729


Train loss: 0.007702909409999847 at step: 106
Iter time:  0.2780634434718006


Train loss: 0.0035125890281051397 at step: 107
Iter time:  0.2780260660938013


Train loss: 0.004722738638520241 at step: 108
Iter time:  0.2779862196357162


Train loss: 0.00858078058809042 at step: 109
Iter time:  0.2779400479902915


Train loss: 0.003941534087061882 at step: 110
Iter time:  0.2779081799767234


Train loss: 0.007420626934617758 at step: 111
Iter time:  0.27787922094534107


Train loss: 0.007099012844264507 at step: 112
Iter time:  0.27785238197871615


Train loss: 0.005632053595036268 at step: 113
Iter time:  0.277823971436087


Train loss: 0.005913127213716507 at step: 114
Iter time:  0.27778700569219755


Train loss: 0.0027262361254543066 at step: 115
Iter time:  0.2777495176895805


Train loss: 0.008727612905204296 at step: 116
Iter time:  0.2777031783399911


Train loss: 0.007795470766723156 at step: 117
Iter time:  0.2776584543733515


Train loss: 0.00421711103990674 at step: 118
Iter time:  0.2776298724998862


Train loss: 0.00978120043873787 at step: 119
Iter time:  0.2775842021493351


Train loss: 0.006877225358039141 at step: 120
Iter time:  0.2775553107261658


Train loss: 0.006353434640914202 at step: 121
Iter time:  0.2775146251867625


Train loss: 0.0051364535465836525 at step: 122
Iter time:  0.27749497773217374


Train loss: 0.00833867583423853 at step: 123
Iter time:  0.27747086005482247


Train loss: 0.009161494672298431 at step: 124
Iter time:  0.27745693345223704


Train loss: 0.004388564266264439 at step: 125
Iter time:  0.2774245357513428


Train loss: 0.009596673771739006 at step: 126
Iter time:  0.277401538122268


Train loss: 0.004350675269961357 at step: 127
Iter time:  0.2773795878793311


Train loss: 0.003770797047764063 at step: 128
Iter time:  0.277389470487833


Train loss: 0.004110480658710003 at step: 129
Iter time:  0.2774320129276246


Train loss: 0.004595249891281128 at step: 130
Iter time:  0.2773991713157067


Train loss: 0.003422702429816127 at step: 131
Iter time:  0.277377920296356


Train loss: 0.0038828908000141382 at step: 132
Iter time:  0.2773522770766056


Train loss: 0.010379096493124962 at step: 133
Iter time:  0.27732532723505693


Train loss: 0.006219449453055859 at step: 134
Iter time:  0.27729924757089186


Train loss: 0.004567820578813553 at step: 135
Iter time:  0.2772903089170103


Train loss: 0.005945703946053982 at step: 136
Iter time:  0.27725865911034975


Train loss: 0.0034552644938230515 at step: 137
Iter time:  0.2772197549360512


Train loss: 0.00707711186259985 at step: 138
Iter time:  0.2771912363992221


Train loss: 0.009362878277897835 at step: 139
Iter time:  0.2771642894195996


Train loss: 0.004475698806345463 at step: 140
Iter time:  0.27715453250067573


Train loss: 0.0030170234385877848 at step: 141
Iter time:  0.27712468221677955


Train loss: 0.008896254003047943 at step: 142
Iter time:  0.27709474865819367


Train loss: 0.005915605463087559 at step: 143
Iter time:  0.2770617491715438


Train loss: 0.003936886787414551 at step: 144
Iter time:  0.27613941000567543


Found 2296 trainable_data in total.
